{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 02 -  Teacher Calibration and Sanity Checks\n",
    "\n",
    "The purpose of this notebook is to calibrate all SMAD teachers (AST, Whisper-AT, Laion CLAP Large, M2D-CLAP) against BLOCS-SMAD-GOLD, then export thresholds for the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "\n",
    "This notebook assumes you have:\n",
    "\n",
    "1. Synced audio and segments with `python scripts/sync_b2_data.py`.\n",
    "2. Built the acoustic-stats manifest: `python -m data_processing.build_acoustic_stats`.\n",
    "3. Run all four teacher scripts: e.g., `python -m data_processing.teachers.apply_ast`.\n",
    "4. Obtain BLOCS-SMAD-GOLD JSONL labels from B2 sync in `data/metadata/`.\n",
    "\n",
    "If you want to tweak thresholds, make a personal copy in `notebooks/local` (e.g., `02_teacher_calibration_<yourname>.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load configuration, gold labels, and teacher outputs\n",
    "\n",
    "+ Configure paths and load all relevant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from utils.config_utils import load_env, add_project_root_to_path\n",
    "load_env()\n",
    "\n",
    "from config import get_settings\n",
    "from utils.dams_types import (\n",
    "    BLOCS_SMAD_V1,\n",
    "    BLOCS_SMAD_V2_AST,\n",
    "    BLOCS_SMAD_V2_CLAP,\n",
    "    BLOCS_SMAD_V2_M2D,\n",
    "    BLOCS_SMAD_V2_WHISPER,\n",
    "    CSV_BLOCS_SMAD_GOLD_ANNOTATIONS\n",
    ")\n",
    "# Ensure project root is in path.\n",
    "add_project_root_to_path()\n",
    "\n",
    "settings = get_settings()\n",
    "metadata_dir: Path = settings.metadata_path\n",
    "print(f\"Metadata dir: {metadata_dir}\")\n",
    "\n",
    "# Load BLOCS SMAD v1 with acoustic stats and QC flags.\n",
    "acoustic_ds = load_from_disk(metadata_dir / BLOCS_SMAD_V1)\n",
    "acoustic_df = acoustic_ds.to_pandas()\n",
    "\n",
    "# Load BLOCS SMAD v2 for each teacher with logits and probabilities.\n",
    "ast_smad_v2 = load_from_disk(metadata_dir / BLOCS_SMAD_V2_AST).to_pandas()\n",
    "clap_smad_v2 = load_from_disk(metadata_dir / BLOCS_SMAD_V2_CLAP).to_pandas()\n",
    "m2d_smad_v2 = load_from_disk(metadata_dir / BLOCS_SMAD_V2_M2D).to_pandas()\n",
    "whisper_smad_v2 = load_from_disk(metadata_dir / BLOCS_SMAD_V2_WHISPER).to_pandas()\n",
    "gold_blocs = pd.read_csv(metadata_dir / CSV_BLOCS_SMAD_GOLD_ANNOTATIONS)\n",
    "\n",
    "print(\"Loaded datasets:\")\n",
    "print(f\"  acoustic_df:    {acoustic_df.shape}\")\n",
    "print(f\"  ast_smad_v2:    {ast_smad_v2.shape}\")\n",
    "print(f\"  clap_smad_v2:   {clap_smad_v2.shape}\")\n",
    "print(f\"  m2d_smad_v2:    {m2d_smad_v2.shape}\")\n",
    "print(f\"  whisper_smad_v2:{whisper_smad_v2.shape}\")\n",
    "print(f\"  gold_smad_v2:   {gold_blocs.shape}\")\n",
    "\n",
    "# Sanity check that all have segment_path for joining\n",
    "for name, df in [\n",
    "    (\"acoustic_df\", acoustic_df),\n",
    "    (\"ast_smad_v2\", ast_smad_v2),\n",
    "    (\"clap_smad_v2\", clap_smad_v2),\n",
    "    (\"m2d_smad_v2\", m2d_smad_v2),\n",
    "    (\"whisper_smad_v2\", whisper_smad_v2),\n",
    "    (\"gold_smad_v2\", gold_blocs),\n",
    "]:\n",
    "    assert \"segment_path\" in df.columns, f\"{name} is missing 'segment_path'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_blocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'speech_gold': 'speech',\n",
    "    'music_gold': 'music',\n",
    "    'noise_gold': 'noise',\n",
    "}\n",
    "\n",
    "for gold_col, base_col in label_map.items():\n",
    "    if gold_col in gold_blocs.columns and base_col not in gold_blocs.columns:\n",
    "        gold_blocs[base_col] = gold_blocs[gold_col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Inspect teacher output schema\n",
    "\n",
    "+ For each teacher, we check that the expected columns are present in the SMAD v2 outputs.\n",
    "+ Verify score fields (probabilities / logits / binary flags) and any existing thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: group all teacher frames in one place\n",
    "teacher_dfs = {\n",
    "    'ast': ast_smad_v2,\n",
    "    'clap': clap_smad_v2,\n",
    "    'm2d': m2d_smad_v2,\n",
    "    'whisper': whisper_smad_v2,\n",
    "}\n",
    "\n",
    "# What we expect from each teacher v2 dataset.\n",
    "# Adjust these lists if your column names differ.\n",
    "teacher_expected_cols = {\n",
    "    'ast': [\n",
    "        'segment_path',\n",
    "        'speech_score', 'music_score', 'noise_score',\n",
    "        'speech_label', 'music_label', 'noise_label',\n",
    "    ],\n",
    "    'clap': [\n",
    "        'segment_path',\n",
    "        'speech_score', 'music_score', 'noise_score',\n",
    "        'speech_label', 'music_label', 'noise_label',\n",
    "    ],\n",
    "    'm2d': [\n",
    "        'segment_path',\n",
    "        'speech_score', 'music_score', 'noise_score',\n",
    "        'speech_label', 'music_label', 'noise_label',\n",
    "    ],\n",
    "    'whisper': [\n",
    "        'segment_path',\n",
    "        'speech_score', 'music_score', 'noise_score',\n",
    "        'speech_label', 'music_label', 'noise_label',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Expected schema for GOLD v2 (HF-style manifest from compute_irr)\n",
    "gold_expected_cols = [\n",
    "    'segment_path',\n",
    "    'raw_file', 'start_time', 'end_time',\n",
    "    'speech', 'music', 'noise',\n",
    "    'n_annotators', 'is_irr_segment',\n",
    "]\n",
    "\n",
    "print('--- GOLD schema check ---')\n",
    "missing_gold = [c for c in gold_expected_cols if c not in gold_blocs.columns]\n",
    "if missing_gold:\n",
    "    print('Missing in gold_smad_v2:', missing_gold)\n",
    "else:\n",
    "    display(gold_blocs[gold_expected_cols].head())\n",
    "    print(gold_blocs[gold_expected_cols].dtypes)\n",
    "\n",
    "print('\\n--- Teacher schema checks ---')\n",
    "for name, df in teacher_dfs.items():\n",
    "    print(f'\\n[{name}] shape={df.shape}')\n",
    "\n",
    "    expected = teacher_expected_cols[name]\n",
    "    missing = [c for c in expected if c not in df.columns]\n",
    "    if missing:\n",
    "        print('  Missing expected columns:', missing)\n",
    "    else:\n",
    "        # Small preview of key columns\n",
    "        display(df[expected].head())\n",
    "        print(df[expected].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Join teachers with gold IRR subset\n",
    "\n",
    "+ Here we restrict labels to `is_irr_segment = True` for calibration on our double-labeled gold subset.\n",
    "+ Quick sanity tables: counts per class in gold vs each teacher’s positives at current thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Build a unified teacher score table with prefixed columns.\n",
    "\n",
    "def make_prefixed_teacher_df(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"Keep segment_path and SMN scores, prefix scores with teacher shortname.\"\"\"\n",
    "    cols = ['segment_path', 'speech_score', 'music_score', 'noise_score']\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{prefix}: missing columns {missing}\")\n",
    "    df_sub = df[cols].copy()\n",
    "    rename_map = {\n",
    "        'speech_score': f'{prefix}_speech_score',\n",
    "        'music_score': f'{prefix}_music_score',\n",
    "        'noise_score': f'{prefix}_noise_score',\n",
    "    }\n",
    "    return df_sub.rename(columns=rename_map)\n",
    "\n",
    "ast_scores = make_prefixed_teacher_df(ast_smad_v2, 'ast')\n",
    "clap_scores = make_prefixed_teacher_df(clap_smad_v2, 'clap')\n",
    "m2d_scores = make_prefixed_teacher_df(m2d_smad_v2, 'm2d_clap')\n",
    "whisper_scores = make_prefixed_teacher_df(whisper_smad_v2, 'whisper')\n",
    "\n",
    "# Start from AST as a base and inner join others on segment_path\n",
    "teacher_blocs = (\n",
    "    ast_scores\n",
    "    .merge(clap_scores, on='segment_path', how='inner')\n",
    "    .merge(m2d_scores, on='segment_path', how='inner')\n",
    "    .merge(whisper_scores, on='segment_path', how='inner')\n",
    ")\n",
    "\n",
    "print('teacher_blocs shape:', teacher_blocs.shape)\n",
    "teacher_blocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict rows to the double-labeled IRR subset.\n",
    "\n",
    "irr_gold = gold_blocs[gold_blocs['is_irr_segment'] == True]\n",
    "print(f\"IRR subset shape: {irr_gold.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. Join IRR gold with all teacher scores.\n",
    "\n",
    "irr_join = irr_gold.merge(teacher_blocs, on='segment_path', how='inner')\n",
    "print(f\"irr_join shape: {irr_join.shape}\")\n",
    "\n",
    "if len(irr_join) != len(irr_gold):\n",
    "    missing = set(irr_gold['segment_path']) - set(teacher_blocs['segment_path'])\n",
    "    print(f\"Warning: {len(missing)} IRR segments missing teacher scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set thresholds for teacher comparison based on prior knowledge or defaults.\n",
    "\n",
    "labels = ['speech', 'music', 'noise']\n",
    "\n",
    "per_teacher_thresholds = {\n",
    "    # AudioSet teachers (AST, Whisper-AT)\n",
    "    'ast': {\n",
    "        'speech': 0.6,\n",
    "        'music': 0.40,\n",
    "        'noise': 0.40,\n",
    "    },\n",
    "    'whisper': {\n",
    "        'speech': 0.60,\n",
    "        'music': 0.40,\n",
    "        'noise': 0.40,\n",
    "    },\n",
    "    # Laion-CLAP zero-shot SMN\n",
    "    'clap': {\n",
    "        'speech': 0.45,\n",
    "        'music': 0.35,\n",
    "        'noise': 0.35,\n",
    "    },\n",
    "    # M2D-CLAP cosine thresholds\n",
    "    'm2d_clap': {\n",
    "        'speech': 0.24,\n",
    "        'music': 0.26,\n",
    "        'noise': 0.24,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Auto discover teacher score columns of the form <teacher>_<label>_score.\n",
    "teacher_score_cols = []\n",
    "for col in irr_join.columns:\n",
    "    # skip gold columns like speech_gold etc.\n",
    "    if col.endswith('_gold'):\n",
    "        continue\n",
    "    for lab in labels:\n",
    "        if col.endswith(f'_{lab}_score'):\n",
    "            teacher_score_cols.append(col)\n",
    "            break\n",
    "\n",
    "# print('Teacher score columns found:')\n",
    "# for c in teacher_score_cols:\n",
    "#     print('  ', c)\n",
    "\n",
    "rows, n_segments = [], len(irr_join)\n",
    "\n",
    "for teacher, thresh_map in per_teacher_thresholds.items():\n",
    "    for label in labels:\n",
    "        score_col = f'{teacher}_{label}_score'\n",
    "        if score_col not in irr_join.columns:\n",
    "            print(f\"Warning: {score_col} not found in irr_join.\")\n",
    "            continue\n",
    "\n",
    "        thr = thresh_map[label]\n",
    "        threshold = thr  # set threshold for teacher and label.\n",
    "        gold_col = f'{label}_gold'\n",
    "        if gold_col not in irr_join.columns:\n",
    "            raise ValueError(f'Missing gold column: {gold_col}')\n",
    "\n",
    "        n_teacher_positive = int((irr_join[score_col] >= threshold).sum())\n",
    "        n_gold_positive = int((irr_join[gold_col] == 1).sum())\n",
    "\n",
    "        rows.append({\n",
    "            'teacher': teacher,\n",
    "            'label': label,\n",
    "            'threshold': threshold,\n",
    "            'n_segments': n_segments,\n",
    "            'n_teacher_positive': n_teacher_positive,\n",
    "            'n_gold_positive': n_gold_positive,\n",
    "            'teacher_pos_rate': n_teacher_positive / n_segments if n_segments > 0 else float('nan'),\n",
    "            'gold_pos_rate': n_gold_positive / n_segments if n_segments > 0 else float('nan'),\n",
    "        })\n",
    "\n",
    "gold_vs_teacher_counts = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values(by=['teacher', 'label'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary table of counts and positive rates.\n",
    "gold_vs_teacher_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Summary of gold vs teacher results\n",
    "\n",
    "On the IRR subset, we have **N = ____** segments. In BLOCS-SMAD-GOLD, the class prevalences are:\n",
    "\n",
    "- Speech: ____ segments (____%)\n",
    "- Music:  ____ segments (____%)\n",
    "- Noise:  ____ segments (____%)\n",
    "\n",
    "At the current working thresholds, the teachers behave as follows:\n",
    "\n",
    "- **AST**\n",
    "  Speech: teacher pos rate ____ vs gold ____\n",
    "  Music:  teacher pos rate ____ vs gold ____\n",
    "  Noise:  teacher pos rate ____ vs gold ____\n",
    "\n",
    "- **Whisper-AT**\n",
    "  Speech: teacher pos rate ____ vs gold ____\n",
    "  Music:  teacher pos rate ____ vs gold ____\n",
    "  Noise:  teacher pos rate ____ vs gold ____\n",
    "\n",
    "- **Laion-CLAP**\n",
    "  Speech: teacher pos rate ____ vs gold ____\n",
    "  Music:  teacher pos rate ____ vs gold ____\n",
    "  Noise:  teacher pos rate ____ vs gold ____\n",
    "\n",
    "- **M2D-CLAP**\n",
    "  Speech: teacher pos rate ____ vs gold ____\n",
    "  Music:  teacher pos rate ____ vs gold ____\n",
    "  Noise:  teacher pos rate ____ vs gold ____\n",
    "\n",
    "Key takeaways in words:\n",
    "\n",
    "- Speech: ___________________________________________\n",
    "- Music:  ___________________________________________\n",
    "- Noise:  ___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 4. Per-class score distributions (IRR subset)\n",
    "\n",
    "+ For each teacher and each class of interest (speech, music, noise), we plot score distributions (probabilities / logits) on positive and negative gold labels.\n",
    "+ This helps us visualize how well each teacher separates positives from negatives, and identify good candidate thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def set_plot_style() -> None:\n",
    "    \"\"\"Set a clean, readable style for EDA plots without grid lines.\"\"\"\n",
    "    sns.set_style(\"white\", {\"axes.grid\": False})\n",
    "    sns.set_context(\"talk\")\n",
    "\n",
    "def style_generic_plot(ax, title: str, xlabel: str, ylabel: str) -> None:\n",
    "    \"\"\"Apply consistent styling to a single axis.\"\"\"\n",
    "    sns.despine(ax=ax)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "    # add horizontal grid lines.\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", labelbottom=True)  # <- force x tick labels on\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "set_plot_style()\n",
    "\n",
    "teachers = list(per_teacher_thresholds.keys())  # ['ast', 'whisper', 'clap', 'm2d_clap']\n",
    "labels = ['speech', 'music', 'noise']\n",
    "\n",
    "def plot_teacher_grid(irr_df: pd.DataFrame, teacher: str) -> None:\n",
    "    \"\"\"\n",
    "    For a single teacher, plot score distributions for speech, music, noise\n",
    "    in a 2×2 grid. Each panel overlays gold=0 and gold=1.\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = 2, 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 7), sharex=True, sharey=False)\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        ax = axes_flat[i]\n",
    "        score_col = f'{teacher}_{label}_score'\n",
    "        gold_col = f'{label}_gold'\n",
    "\n",
    "        if score_col not in irr_df.columns:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # Make a tidy frame for seaborn\n",
    "        tmp = irr_df[[score_col, gold_col]].copy()\n",
    "        tmp = tmp.rename(columns={score_col: 'score', gold_col: 'gold'})\n",
    "        tmp['gold'] = tmp['gold'].map({0: 'gold=0', 1: 'gold=1'})\n",
    "\n",
    "        sns.histplot(\n",
    "            data=tmp,\n",
    "            x='score',\n",
    "            hue='gold',\n",
    "            bins=20,\n",
    "            stat='density',\n",
    "            element='step',   # outlines + fill\n",
    "            alpha=0.7,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        thr = per_teacher_thresholds[teacher][label]\n",
    "        ax.axvline(thr, linestyle='--', linewidth=1.5, label=f'thr={thr:.2f}')\n",
    "\n",
    "        style_generic_plot(\n",
    "            ax,\n",
    "            title=f'{teacher} – {label} scores on IRR',\n",
    "            xlabel='score',\n",
    "            ylabel='density',\n",
    "        )\n",
    "\n",
    "        # Avoid duplicate legends in every panel: keep only top left.\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.legend().remove()\n",
    "\n",
    "    # Turn off any unused subplot (bottom right, since we only have 3 labels).\n",
    "    if len(labels) < len(axes_flat):\n",
    "        for j in range(len(labels), len(axes_flat)):\n",
    "            axes_flat[j].axis('off')\n",
    "\n",
    "    fig.suptitle(f'{teacher} score distributions on IRR', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example: loop teachers, one 2×2 grid per teacher\n",
    "for t in teachers:\n",
    "    plot_teacher_grid(irr_join, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Summary of score distribution observations\n",
    "\n",
    "- **AST**\n",
    "    + Speech: ___________________________________________\n",
    "    + Music:  ___________________________________________\n",
    "    + Noise:  ___________________________________________\n",
    "- **Whisper-AT**\n",
    "    + Speech: ___________________________________________\n",
    "    + Music:  ___________________________________________\n",
    "    + Noise:  ___________________________________________\n",
    "- **Laion-CLAP**\n",
    "    + Speech: ___________________________________________\n",
    "    + Music:  ___________________________________________\n",
    "    + Noise:  ___________________________________________\n",
    "- **M2D-CLAP**\n",
    "    + Speech: ___________________________________________\n",
    "    + Music:  ___________________________________________\n",
    "    + Noise:  ___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 5. Threshold sweeps and calibration curves\n",
    "\n",
    "+ For each teacher and class, we sweep over possible decision thresholds and plot precision-recall and ROC curves on the IRR subset.\n",
    "+ This helps us pick operating points that balance false positives and false negatives according to our needs.\n",
    "+ We record candidate thresholds that match desired precision or recall levels. (e.g., 90% precision on speech, 80% recall on music, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Imports and setup\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def compute_curves(df, teacher: str, label: str) -> tuple:\n",
    "    \"\"\"Compute precision-recall and ROC curves for a given teacher and label.\"\"\"\n",
    "    score_col = f'{teacher}_{label}_score'\n",
    "    gold_col = f'{label}_gold'\n",
    "\n",
    "    y_true = df[gold_col].astype(int).values\n",
    "    y_score = df[score_col].astype(float).values\n",
    "\n",
    "    # Precision Recall\n",
    "    prec, rec, pr_thr = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, roc_thr = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return prec, rec, pr_thr, fpr, tpr, roc_thr, roc_auc\n",
    "\n",
    "\n",
    "def plot_teacher_curves(df, teacher: str):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    for label in labels:\n",
    "        score_col = f\"{teacher}_{label}_score\"\n",
    "        if score_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        prec, rec, pr_thr, fpr, tpr, roc_thr, roc_auc = compute_curves(df, teacher, label)\n",
    "\n",
    "        # --- PR curve ---\n",
    "        ax = axes[0]\n",
    "        ax.plot(rec, prec, label=f'{label}')\n",
    "        ax.set_title(f'{teacher}: Precision–Recall')\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.grid(False)\n",
    "\n",
    "        # --- ROC curve ---\n",
    "        ax = axes[1]\n",
    "        ax.plot(fpr, tpr, label=f'{label} (AUC={roc_auc:.2f})')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "        ax.set_title(f'{teacher}: ROC Curve')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.grid(False)\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    fig.suptitle(f'{teacher} — threshold calibration curves', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Plot curves for each teacher.\n",
    "\n",
    "for t in teachers:\n",
    "    plot_teacher_curves(irr_join, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ 5.3. Record candidate thresholds based on desired precision/recall.\n",
    "\n",
    "def find_threshold_for_precision(df, teacher, label, target_precision=0.90):\n",
    "    prec, rec, thr = precision_recall_curve(\n",
    "        df[f'{label}_gold'].astype(int),\n",
    "        df[f'{teacher}_{label}_score']\n",
    "    )\n",
    "\n",
    "    # Remove the last \"None\" threshold sklearn appends\n",
    "    thr = thr[:-1]\n",
    "\n",
    "    candidates = np.where(prec[:-1] >= target_precision)[0]\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "\n",
    "    idx = candidates[0]\n",
    "    return float(thr[idx]), float(prec[idx]), float(rec[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4. Test for candidate thresholds at target precision for each teacher and label.\n",
    "\n",
    "find_threshold_for_precision(irr_join, 'ast', 'speech', target_precision=0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 5.5. Threshold Calibration Curves\n",
    "\n",
    "This section sweeps thresholds from 0 to 1 and examines precision, recall, and ROC behavior on the IRR subset.\n",
    "\n",
    "**Speech**:\n",
    "+ Teacher achieving highest AUC: ____\n",
    "+ Threshold achieving ≥90 percent precision: ____\n",
    "+ Tradeoff comment: ____*\n",
    "\n",
    "**Music**:\n",
    "+ Teacher with most stable PR curve: ____\n",
    "+ Threshold achieving ≥80 percent recall: ____*\n",
    "\n",
    "**Noise**:\n",
    "+ Most reliable teacher: ____\n",
    "+ Threshold achieving balanced precision and recall (or best F1): ____*\n",
    "\n",
    "Candidate Operating Points\n",
    "\n",
    "Proposed thresholds derived from PR sweeps:\n",
    "+ speech: ____\n",
    "+ music: ____\n",
    "+ noise: ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 6. Compare working vs calibrated thresholds\n",
    "\n",
    "+ For each teacher and class, we compare the current working thresholds (if any) against the newly calibrated thresholds from the previous section.\n",
    "+ We analyze how the new thresholds would change positive rates and error patterns on the IRR subset.\n",
    "+ Then we finalize threshold choices and document them for pipeline integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Set new thresholds based on calibration findings.\n",
    "calibrated_thresholds = {\n",
    "    'ast': {'speech': 0.57, 'music': 0.43, 'noise': 0.41},\n",
    "    'whisper': {'speech': 0.61, 'music': 0.39, 'noise': 0.38},\n",
    "    'clap': {'speech': 0.46, 'music': 0.34, 'noise': 0.33},\n",
    "    'm2d_clap': {'speech': 0.25, 'music': 0.27, 'noise': 0.25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Compare working vs calibrated thresholds.\n",
    "\n",
    "def compute_pos_rate(df: pd.DataFrame, teacher: str, label: str, threshold: float):\n",
    "    \"\"\"Compute positive count and rate for a given teacher/label/threshold on irr_join.\"\"\"\n",
    "    score_col = f'{teacher}_{label}_score'\n",
    "    if score_col not in df.columns:\n",
    "        return None, None\n",
    "    mask = df[score_col] >= threshold\n",
    "    n_pos = int(mask.sum())\n",
    "    rate = float(mask.mean()) if len(df) > 0 else float('nan')\n",
    "    return n_pos, rate\n",
    "\n",
    "\n",
    "# Sanity check that calibrated_thresholds exists.\n",
    "try:\n",
    "    calibrated_thresholds\n",
    "except NameError as e:\n",
    "    raise ValueError(\n",
    "        \"calibrated_thresholds is not defined. \"\n",
    "        \"Please create a dict like calibrated_thresholds[teacher][label] = new_threshold \"\n",
    "        \"based on your PR/ROC sweeps in Section 5.\"\n",
    "    ) from e\n",
    "\n",
    "rows = []\n",
    "n_segments_irr = len(irr_join)\n",
    "\n",
    "for teacher, old_map in per_teacher_thresholds.items():\n",
    "    new_map = calibrated_thresholds.get(teacher, {})\n",
    "\n",
    "    for label in labels:\n",
    "        old_thr = old_map.get(label, None)\n",
    "        new_thr = new_map.get(label, None)\n",
    "\n",
    "        # Skip if we do not have scores for this teacher/label.\n",
    "        score_col = f'{teacher}_{label}_score'\n",
    "        if score_col not in irr_join.columns:\n",
    "            continue\n",
    "\n",
    "        # Old threshold stats\n",
    "        old_n_pos, old_rate = (None, None)\n",
    "        if old_thr is not None:\n",
    "            old_n_pos, old_rate = compute_pos_rate(irr_join, teacher, label, old_thr)\n",
    "\n",
    "        # New threshold stats\n",
    "        new_n_pos, new_rate = (None, None)\n",
    "        if new_thr is not None:\n",
    "            new_n_pos, new_rate = compute_pos_rate(irr_join, teacher, label, new_thr)\n",
    "\n",
    "        rows.append({\n",
    "            'teacher': teacher,\n",
    "            'label': label,\n",
    "            'old_threshold': old_thr,\n",
    "            'new_threshold': new_thr,\n",
    "            'delta_threshold': (\n",
    "                None if (old_thr is None or new_thr is None)\n",
    "                else float(new_thr - old_thr)\n",
    "            ),\n",
    "            'n_segments_irr': n_segments_irr,\n",
    "            'old_n_positive': old_n_pos,\n",
    "            'new_n_positive': new_n_pos,\n",
    "            'old_pos_rate': old_rate,\n",
    "            'new_pos_rate': new_rate,\n",
    "            'delta_pos_rate': (\n",
    "                None if (old_rate is None or new_rate is None)\n",
    "                else float(new_rate - old_rate)\n",
    "            ),\n",
    "        })\n",
    "\n",
    "threshold_delta_table = (\n",
    "    pd.DataFrame(rows)\n",
    "    .sort_values(['teacher', 'label'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "threshold_delta_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 7. Qualitative spot checks of teacher-gold disagreements (optional)\n",
    "\n",
    "+ Sample sample segments where each teacher strongly disagrees with gold (false positives / false negatives) at the chosen thresholds.\n",
    "+ Show scores, labels, and play audio; jot down typical failure modes and edge cases for each teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 8. Export final thresholds and summary (optional)\n",
    "\n",
    "+ We compile the final chosen thresholds for each teacher and class into a summary table.\n",
    "+ Export this table to a CSV or JSON file for easy reference during pipeline implementation.\n",
    "+ Summarize key findings and recommendations for future teacher calibration efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
