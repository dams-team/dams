{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ce204d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad7c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata directory: /Users/benji/Desktop/columbia/dams/data/metadata\n",
      "Segments directory: /Users/benji/Desktop/columbia/dams/data/segments\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Ensure notebook can import project modules\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"utils\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.config_utils import load_env, add_project_root_to_path\n",
    "load_env()\n",
    "\n",
    "from config import get_settings, AST_MODEL_NAME, SAMPLE_RATE\n",
    "from utils.dams_types import (\n",
    "    SPEECH,\n",
    "    MUSIC,\n",
    "    NOISE,\n",
    "    SPEECH_SCORE,\n",
    "    MUSIC_SCORE,\n",
    "    NOISE_SCORE,\n",
    "    SEGMENT_PATH,\n",
    "    BLOCS_SMAD_V2_M2D,\n",
    "    BLOCS_SMAD_V2_AST,\n",
    "    BLOCS_SMAD_V2_CLAP,\n",
    "    BLOCS_SMAD_V2_WHISPER,\n",
    ")\n",
    "from utils.audio_io import load_mono_resampled\n",
    "\n",
    "# Ensure project root is in path\n",
    "add_project_root_to_path()\n",
    "\n",
    "settings = get_settings()\n",
    "metadata_dir = Path(settings.metadata_path)\n",
    "segments_dir = Path(settings.segments_path)\n",
    "\n",
    "print(f\"Metadata directory: {metadata_dir}\")\n",
    "print(f\"Segments directory: {segments_dir}\")\n",
    "\n",
    "# Device setup\n",
    "def get_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aea5a4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Change the CSV file here to switch data sources.** The CSV file should be in `data/metadata/` directory.\n",
    "\n",
    "The notebook supports two formats:\n",
    "1. **New format** (recommended): `blocs_smad_v2_finetune.csv` with `chosen_speech_label`, `chosen_music_label`, `chosen_noise_label` columns\n",
    "2. **Old format**: Files like `blocs_smad_v2_m2d.csv`, `blocs_smad_v2_ast.csv`, etc. with `speech_label`, `music_label`, `noise_label` columns\n",
    "\n",
    "The notebook will automatically detect which format you're using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0cec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune.csv\n",
      "Loaded 6196 samples\n",
      "Columns: ['segment_path', 'ast_speech_score', 'ast_music_score', 'ast_noise_score', 'ast_speech_label', 'ast_music_label', 'ast_noise_label', 'ast_ast_probs', 'clap_speech_score', 'clap_music_score', 'clap_noise_score', 'clap_speech_label', 'clap_music_label', 'clap_noise_label', 'm2d_speech_score', 'm2d_music_score', 'm2d_noise_score', 'm2d_speech_label', 'm2d_music_label', 'm2d_noise_label', 'm2d_m2d_clap_scores', 'whisper_speech_score', 'whisper_music_score', 'whisper_noise_score', 'whisper_speech_label', 'whisper_music_label', 'whisper_noise_label', 'whisper_whisper_probs', 'chosen_speech_label', 'chosen_speech_score', 'chosen_music_label', 'chosen_music_score', 'chosen_noise_label', 'chosen_noise_score']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_path</th>\n",
       "      <th>ast_speech_score</th>\n",
       "      <th>ast_music_score</th>\n",
       "      <th>ast_noise_score</th>\n",
       "      <th>ast_speech_label</th>\n",
       "      <th>ast_music_label</th>\n",
       "      <th>ast_noise_label</th>\n",
       "      <th>ast_ast_probs</th>\n",
       "      <th>clap_speech_score</th>\n",
       "      <th>clap_music_score</th>\n",
       "      <th>...</th>\n",
       "      <th>whisper_speech_label</th>\n",
       "      <th>whisper_music_label</th>\n",
       "      <th>whisper_noise_label</th>\n",
       "      <th>whisper_whisper_probs</th>\n",
       "      <th>chosen_speech_label</th>\n",
       "      <th>chosen_speech_score</th>\n",
       "      <th>chosen_music_label</th>\n",
       "      <th>chosen_music_score</th>\n",
       "      <th>chosen_noise_label</th>\n",
       "      <th>chosen_noise_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001_NO_RAD_0001_s0001.wav</td>\n",
       "      <td>0.081762</td>\n",
       "      <td>0.801188</td>\n",
       "      <td>0.255025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[3.63510922e-02 4.56079113e-04 2.27431403e-04 ...</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.997437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.92587660e-03 2.62785441e-04 1.19219636e-04 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155655</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001_NO_RAD_0001_s0002.wav</td>\n",
       "      <td>0.090267</td>\n",
       "      <td>0.828630</td>\n",
       "      <td>0.248389</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.04750444e-02 7.17045390e-04 3.32226511e-04 ...</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.997065</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.89507976e-04 1.54336740e-04 6.88402215e-05 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154187</td>\n",
       "      <td>1</td>\n",
       "      <td>0.828630</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001_NO_RAD_0001_s0003.wav</td>\n",
       "      <td>0.127166</td>\n",
       "      <td>0.770716</td>\n",
       "      <td>0.274406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.19382825e-02 3.93654394e-04 2.33048966e-04 ...</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.996777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.20434696e-04 1.81718293e-04 4.76580535e-05 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152717</td>\n",
       "      <td>1</td>\n",
       "      <td>0.770716</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001_NO_RAD_0001_s0004.wav</td>\n",
       "      <td>0.235577</td>\n",
       "      <td>0.874629</td>\n",
       "      <td>0.178180</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.17415676e-01 1.74700480e-03 3.13305034e-04 ...</td>\n",
       "      <td>0.096676</td>\n",
       "      <td>0.903154</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.08439961e-01 1.82499515e-03 1.74823828e-04 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.201432</td>\n",
       "      <td>1</td>\n",
       "      <td>0.874629</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001_NO_RAD_0001_s0005.wav</td>\n",
       "      <td>0.509043</td>\n",
       "      <td>0.824807</td>\n",
       "      <td>0.161153</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.72871393e-01 6.19728817e-03 4.72094025e-03 ...</td>\n",
       "      <td>0.069654</td>\n",
       "      <td>0.929963</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.09234643e-01 5.35355462e-03 2.50797067e-03 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.244172</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824807</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                segment_path  ast_speech_score  ast_music_score  \\\n",
       "0  001_NO_RAD_0001_s0001.wav          0.081762         0.801188   \n",
       "1  001_NO_RAD_0001_s0002.wav          0.090267         0.828630   \n",
       "2  001_NO_RAD_0001_s0003.wav          0.127166         0.770716   \n",
       "3  001_NO_RAD_0001_s0004.wav          0.235577         0.874629   \n",
       "4  001_NO_RAD_0001_s0005.wav          0.509043         0.824807   \n",
       "\n",
       "   ast_noise_score  ast_speech_label  ast_music_label  ast_noise_label  \\\n",
       "0         0.255025                 0                1                0   \n",
       "1         0.248389                 0                1                0   \n",
       "2         0.274406                 0                1                0   \n",
       "3         0.178180                 0                1                0   \n",
       "4         0.161153                 0                1                0   \n",
       "\n",
       "                                       ast_ast_probs  clap_speech_score  \\\n",
       "0  [3.63510922e-02 4.56079113e-04 2.27431403e-04 ...           0.002559   \n",
       "1  [4.04750444e-02 7.17045390e-04 3.32226511e-04 ...           0.002932   \n",
       "2  [4.19382825e-02 3.93654394e-04 2.33048966e-04 ...           0.003219   \n",
       "3  [2.17415676e-01 1.74700480e-03 3.13305034e-04 ...           0.096676   \n",
       "4  [4.72871393e-01 6.19728817e-03 4.72094025e-03 ...           0.069654   \n",
       "\n",
       "   clap_music_score  ...  whisper_speech_label  whisper_music_label  \\\n",
       "0          0.997437  ...                     0                    1   \n",
       "1          0.997065  ...                     0                    1   \n",
       "2          0.996777  ...                     0                    1   \n",
       "3          0.903154  ...                     0                    1   \n",
       "4          0.929963  ...                     0                    1   \n",
       "\n",
       "   whisper_noise_label                              whisper_whisper_probs  \\\n",
       "0                    0  [1.92587660e-03 2.62785441e-04 1.19219636e-04 ...   \n",
       "1                    0  [4.89507976e-04 1.54336740e-04 6.88402215e-05 ...   \n",
       "2                    0  [5.20434696e-04 1.81718293e-04 4.76580535e-05 ...   \n",
       "3                    0  [2.08439961e-01 1.82499515e-03 1.74823828e-04 ...   \n",
       "4                    0  [4.09234643e-01 5.35355462e-03 2.50797067e-03 ...   \n",
       "\n",
       "   chosen_speech_label  chosen_speech_score  chosen_music_label  \\\n",
       "0                    0             0.155655                   1   \n",
       "1                    0             0.154187                   1   \n",
       "2                    0             0.152717                   1   \n",
       "3                    0             0.201432                   1   \n",
       "4                    1             0.244172                   1   \n",
       "\n",
       "   chosen_music_score  chosen_noise_label  chosen_noise_score  \n",
       "0            0.801188                   0            0.000004  \n",
       "1            0.828630                   0            0.000003  \n",
       "2            0.770716                   0            0.000003  \n",
       "3            0.874629                   0            0.000169  \n",
       "4            0.824807                   0            0.000383  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION: Change this to switch CSV files\n",
    "# ============================================\n",
    "CSV_FILE = \"blocs_smad_v2_finetune.csv\"  # Change this to your desired CSV file\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 4  # Effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "WEIGHT_DECAY = 0.01\n",
    "TRAIN_SPLIT = 0.8  # 80% train, 20% validation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model configuration\n",
    "NUM_CLASSES = 3  # [speech, music, noise]\n",
    "FREEZE_ENCODER = False  # Set to True to freeze AST encoder and only train head\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = metadata_dir / CSV_FILE\n",
    "print(f\"Loading data from: {csv_path}\")\n",
    "\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a18c3",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Create a PyTorch Dataset class that loads audio segments and their labels from the CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f160613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor initialized\n"
     ]
    }
   ],
   "source": [
    "class PseudolabelDataset(Dataset):\n",
    "    \"\"\"Dataset for loading audio segments and multilabel targets from CSV.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        segments_dir: Path,\n",
    "        feature_extractor: AutoFeatureExtractor,\n",
    "        use_scores: bool = False,\n",
    "        use_chosen_labels: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns: segment_path, chosen_speech_label, chosen_music_label, chosen_noise_label\n",
    "                (or speech_label, music_label, noise_label for old format)\n",
    "            segments_dir: Directory containing audio segment files\n",
    "            feature_extractor: AST feature extractor for preprocessing\n",
    "            use_scores: If True, use continuous scores instead of binary labels\n",
    "            use_chosen_labels: If True, use chosen_* columns (new format), else use old format columns\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.segments_dir = segments_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.use_scores = use_scores\n",
    "        self.use_chosen_labels = use_chosen_labels\n",
    "        \n",
    "        # Determine column names based on format\n",
    "        if use_chosen_labels:\n",
    "            self.speech_col = 'chosen_speech_label'\n",
    "            self.music_col = 'chosen_music_label'\n",
    "            self.noise_col = 'chosen_noise_label'\n",
    "            self.speech_score_col = 'chosen_speech_score'\n",
    "            self.music_score_col = 'chosen_music_score'\n",
    "            self.noise_score_col = 'chosen_noise_score'\n",
    "        else:\n",
    "            # Fallback to old format\n",
    "            self.speech_col = SPEECH\n",
    "            self.music_col = MUSIC\n",
    "            self.noise_col = NOISE\n",
    "            self.speech_score_col = SPEECH_SCORE\n",
    "            self.music_score_col = MUSIC_SCORE\n",
    "            self.noise_score_col = NOISE_SCORE\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = [SEGMENT_PATH, self.speech_col, self.music_col, self.noise_col]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        segment_path = self.segments_dir / row[SEGMENT_PATH]\n",
    "        \n",
    "        # Load audio\n",
    "        waveform = load_mono_resampled(segment_path, target_sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Preprocess with feature extractor\n",
    "        inputs = self.feature_extractor(\n",
    "            waveform.numpy(),\n",
    "            sampling_rate=SAMPLE_RATE,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        \n",
    "        # Extract labels\n",
    "        if self.use_scores and all(col in row for col in [self.speech_score_col, self.music_score_col, self.noise_score_col]):\n",
    "            labels = torch.tensor([\n",
    "                row[self.speech_score_col],\n",
    "                row[self.music_score_col],\n",
    "                row[self.noise_score_col]\n",
    "            ], dtype=torch.float32)\n",
    "        else:\n",
    "            labels = torch.tensor([\n",
    "                int(row[self.speech_col]),\n",
    "                int(row[self.music_col]),\n",
    "                int(row[self.noise_col])\n",
    "            ], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs['input_values'].squeeze(0),\n",
    "            'labels': labels,\n",
    "            'segment_path': row[SEGMENT_PATH]\n",
    "        }\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(AST_MODEL_NAME)\n",
    "print(\"Feature extractor initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7734635",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Create an AST-based encoder with a custom classification head for 3-class multilabel classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e67a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training full model (encoder + head)\n",
      "Model initialized on mps\n",
      "Total parameters: 86,191,107\n",
      "Trainable parameters: 86,191,107\n"
     ]
    }
   ],
   "source": [
    "class ASTStudentClassifier(nn.Module):\n",
    "    \"\"\"AST encoder with custom classification head for multilabel classification.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = AST_MODEL_NAME,\n",
    "        num_classes: int = 3,\n",
    "        freeze_encoder: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained AST model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.encoder = AutoModelForAudioClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        # Get the hidden size from the encoder\n",
    "        # AST models typically have a classifier head, we'll replace it\n",
    "        hidden_size = config.hidden_size\n",
    "        \n",
    "        # Remove the original classification head (if it exists)\n",
    "        if hasattr(self.encoder, 'classifier'):\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif hasattr(self.encoder, 'projector'):\n",
    "            self.encoder.projector = nn.Identity()\n",
    "        \n",
    "        # Create new classification head for 3-class multilabel\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Freeze encoder if requested\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Encoder frozen, only training classification head\")\n",
    "        else:\n",
    "            print(\"Training full model (encoder + head)\")\n",
    "    \n",
    "    def forward(self, input_values):\n",
    "        # Get encoder outputs with output_hidden_states to access features\n",
    "        outputs = self.encoder(\n",
    "            input_values,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        # Extract pooled features from the last hidden state\n",
    "        # AST models have hidden_states in the outputs when output_hidden_states=True\n",
    "        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "            # Use last hidden state (from the transformer encoder)\n",
    "            last_hidden_state = outputs.hidden_states[-1]  # [batch, seq_len, hidden_size]\n",
    "            # Average pooling over sequence dimension\n",
    "            pooled = last_hidden_state.mean(dim=1)  # [batch, hidden_size]\n",
    "        else:\n",
    "            # Fallback: access the base transformer model directly\n",
    "            # AST models have audio_spectrogram_transformer attribute\n",
    "            if hasattr(self.encoder, 'audio_spectrogram_transformer'):\n",
    "                transformer = self.encoder.audio_spectrogram_transformer\n",
    "                # Get embeddings\n",
    "                embeddings = transformer.embeddings(input_values)\n",
    "                # Pass through encoder\n",
    "                encoder_outputs = transformer.encoder(embeddings, output_hidden_states=True)\n",
    "                # Get last hidden state and pool\n",
    "                last_hidden_state = encoder_outputs.last_hidden_state\n",
    "                pooled = last_hidden_state.mean(dim=1)\n",
    "            else:\n",
    "                # Last resort: try to get features from model outputs\n",
    "                # Some AST models might have different structure\n",
    "                raise NotImplementedError(\n",
    "                    f\"Could not extract features from AST model. \"\n",
    "                    f\"Model structure: {type(self.encoder)}. \"\n",
    "                    f\"Available attributes: {dir(self.encoder)}\"\n",
    "                )\n",
    "        \n",
    "        # Apply classification head\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = ASTStudentClassifier(\n",
    "    model_name=AST_MODEL_NAME,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    freeze_encoder=FREEZE_ENCODER,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44710fa",
   "metadata": {},
   "source": [
    "## Prepare Data Splits\n",
    "\n",
    "Split the data into train and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93dc4287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new format: chosen_* columns\n",
      "After filtering: 6196 samples\n",
      "Train samples: 4956\n",
      "Validation samples: 1240\n",
      "Data loaders created\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with missing data\n",
    "# Check which format we're using (new format with chosen_* or old format)\n",
    "if 'chosen_speech_label' in df.columns:\n",
    "    label_cols = ['chosen_speech_label', 'chosen_music_label', 'chosen_noise_label']\n",
    "    use_chosen = True\n",
    "    print(\"Using new format: chosen_* columns\")\n",
    "else:\n",
    "    label_cols = [SPEECH, MUSIC, NOISE]\n",
    "    use_chosen = False\n",
    "    print(\"Using old format: speech_label, music_label, noise_label\")\n",
    "\n",
    "df_clean = df.dropna(subset=[SEGMENT_PATH] + label_cols)\n",
    "print(f\"After filtering: {len(df_clean)} samples\")\n",
    "\n",
    "# Split into train and validation\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "indices = np.random.permutation(len(df_clean))\n",
    "split_idx = int(len(df_clean) * TRAIN_SPLIT)\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:]\n",
    "\n",
    "df_train = df_clean.iloc[train_indices].reset_index(drop=True)\n",
    "df_val = df_clean.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(df_train)}\")\n",
    "print(f\"Validation samples: {len(df_val)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PseudolabelDataset(\n",
    "    df_train,\n",
    "    segments_dir,\n",
    "    feature_extractor,\n",
    "    use_scores=False,  # Use binary labels\n",
    "    use_chosen_labels=use_chosen,  # Use chosen_* format if available\n",
    ")\n",
    "\n",
    "val_dataset = PseudolabelDataset(\n",
    "    df_val,\n",
    "    segments_dir,\n",
    "    feature_extractor,\n",
    "    use_scores=False,\n",
    "    use_chosen_labels=use_chosen,  # Use chosen_* format if available\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for compatibility, increase if needed\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False,\n",
    ")\n",
    "\n",
    "print(\"Data loaders created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890d6d6",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Initialize loss function, optimizer, and scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404489b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete\n",
      "Loss function: BCEWithLogitsLoss()\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x30052de80>\n"
     ]
    }
   ],
   "source": [
    "# Loss function: BCEWithLogitsLoss for multilabel classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=NUM_EPOCHS,\n",
    "    eta_min=LEARNING_RATE * 0.01,\n",
    ")\n",
    "\n",
    "print(\"Training setup complete\")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: {scheduler}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa4b14",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the model with validation monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▊         | 433/4956 [06:47<1:10:51,  1.06it/s, loss=0.0014, acc=1.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Gradient accumulation to control memory on MPS\u001b[39;00m\n\u001b[32m     65\u001b[39m loss = loss / GRAD_ACCUM_STEPS\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % GRAD_ACCUM_STEPS == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (step + \u001b[32m1\u001b[39m) == \u001b[38;5;28mlen\u001b[39m(train_loader):\n\u001b[32m     69\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "\n",
    "def compute_accuracy(logits, labels, threshold=0.5):\n",
    "    \"\"\"Compute multilabel accuracy with threshold.\"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= threshold).float()\n",
    "    # Exact match accuracy (all labels must match)\n",
    "    correct = (preds == labels).all(dim=1).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Run validation.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_values)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += compute_accuracy(logits, labels)\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_acc / num_batches\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_values)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Gradient accumulation to control memory on MPS\n",
    "        loss = loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0 or (step + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Metrics (use unscaled loss for reporting)\n",
    "        train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "        train_acc += compute_accuracy(logits, labels)\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{(loss.item() * GRAD_ACCUM_STEPS):.4f}\",\n",
    "            'acc': f\"{compute_accuracy(logits, labels):.4f}\"\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = train_loss / num_batches\n",
    "    avg_train_acc = train_acc / num_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    avg_val_loss, avg_val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['train_acc'].append(avg_train_acc)\n",
    "    history['val_acc'].append(avg_val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model (you can add model saving here)\n",
    "        print('✓ New best validation loss!')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('Training complete!')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec66755",
   "metadata": {},
   "source": [
    "## Training Curves\n",
    "\n",
    "Visualize training and validation loss/accuracy over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8fe07",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the model on the validation set with detailed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9aa8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/78 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.79 GiB, other allocations: 2.11 GiB, max allowed: 9.07 GiB). Tried to allocate 227.62 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     54\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m: all_preds,\n\u001b[32m     55\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: all_labels,\n\u001b[32m     56\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mprobabilities\u001b[39m\u001b[33m'\u001b[39m: all_probs,\n\u001b[32m     57\u001b[39m     }\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m eval_results = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, val_loader, device, threshold)\u001b[39m\n\u001b[32m     18\u001b[39m input_values = batch[\u001b[33m'\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     19\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m probs = torch.sigmoid(logits)\n\u001b[32m     23\u001b[39m preds = (probs >= threshold).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mASTStudentClassifier.forward\u001b[39m\u001b[34m(self, input_values)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_values):\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Get encoder outputs with output_hidden_states to access features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Extract pooled features from the last hidden state\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# AST models have hidden_states in the outputs when output_hidden_states=True\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m outputs.hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m# Use last hidden state (from the transformer encoder)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:462\u001b[39m, in \u001b[36mASTForAudioClassification.forward\u001b[39m\u001b[34m(self, input_values, head_mask, labels, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    448\u001b[39m ) -> SequenceClassifierOutput:\n\u001b[32m    449\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03m    input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m        Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    460\u001b[39m \u001b[33;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maudio_spectrogram_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m     pooled_output = outputs.pooler_output\n\u001b[32m    467\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:400\u001b[39m, in \u001b[36mASTModel.forward\u001b[39m\u001b[34m(self, input_values, head_mask, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    398\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_values)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m sequence_output = encoder_outputs.last_hidden_state\n\u001b[32m    402\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.layernorm(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:305\u001b[39m, in \u001b[36mASTEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m    304\u001b[39m     layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(last_hidden_state=hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1031\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1029\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1033\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1031\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1029\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1033\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1031\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1029\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1033\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:286\u001b[39m, in \u001b[36mASTLayer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# in AST, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[32m    285\u001b[39m layer_output = \u001b[38;5;28mself\u001b[39m.layernorm_after(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n\u001b[32m    289\u001b[39m layer_output = \u001b[38;5;28mself\u001b[39m.output(layer_output, hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:245\u001b[39m, in \u001b[36mASTIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m    244\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/.venv/lib/python3.13/site-packages/transformers/activations.py:85\u001b[39m, in \u001b[36mGELUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 6.79 GiB, other allocations: 2.11 GiB, max allowed: 9.07 GiB). Tried to allocate 227.62 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    multilabel_confusion_matrix,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    ")\n",
    "\n",
    "def evaluate_model(model, val_loader, device, threshold=0.5):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_values)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= threshold).float()\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    \n",
    "    # Classification report\n",
    "    class_names = ['speech', 'music', 'noise']\n",
    "    print(\"Classification Report (per class):\")\n",
    "    print(classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Multilabel metrics\n",
    "    print(f\"\\nHamming Loss: {hamming_loss(all_labels, all_preds):.4f}\")\n",
    "    print(f\"Jaccard Score (micro): {jaccard_score(all_labels, all_preds, average='micro'):.4f}\")\n",
    "    print(f\"Jaccard Score (macro): {jaccard_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "    print(f\"Jaccard Score (per class): {jaccard_score(all_labels, all_preds, average=None)}\")\n",
    "    \n",
    "    # Exact match accuracy\n",
    "    exact_match = (all_preds == all_labels).all(axis=1).mean()\n",
    "    print(f\"\\nExact Match Accuracy: {exact_match:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs,\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4a766",
   "metadata": {},
   "source": [
    "## Save Model (Optional)\n",
    "\n",
    "Save the trained model for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b15dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model\n",
    "# model_save_path = settings.models_path / f\"ast_student_{CSV_FILE.replace('.csv', '')}.pt\"\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'history': history,\n",
    "#     'config': {\n",
    "#         'num_classes': NUM_CLASSES,\n",
    "#         'freeze_encoder': FREEZE_ENCODER,\n",
    "#         'csv_file': CSV_FILE,\n",
    "#     }\n",
    "# }, model_save_path)\n",
    "# print(f\"Model saved to: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f54df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
