{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c832df5",
   "metadata": {},
   "source": [
    "\n",
    "# Finetune SMAD Student (AST) End-to-End\n",
    "\n",
    "This notebook builds the fused pseudo-label manifest, validates it, trains the AST student, and evaluates on gold.\n",
    "\n",
    "**Assumptions**\n",
    "- Run from within the repo; audio segments live in `data/segments/`.\n",
    "- Dependencies installed: torch, torchaudio, transformers, pandas, datasets, scikit-learn.\n",
    "- Teachers HF datasets are under `data/metadata/blocs_smad_v2_*`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121b9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /Users/benji/Desktop/columbia/dams\n",
      "Working dir set to: /Users/benji/Desktop/columbia/dams\n"
     ]
    }
   ],
   "source": [
    "# Resolve project root and set paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / 'data').exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "import os\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f'Working dir set to: {Path.cwd()}')\n",
    "\n",
    "METADATA_DIR = PROJECT_ROOT / 'data/metadata'\n",
    "SEGMENTS_DIR = PROJECT_ROOT / 'data/segments'\n",
    "MANIFEST = METADATA_DIR / 'blocs_smad_v2_finetune.csv'\n",
    "GOLD = METADATA_DIR / 'blocs_smad_gold_annotations_v1.csv'\n",
    "CHECKPOINT = PROJECT_ROOT / 'checkpoints/student_ast.pt'\n",
    "CHECKPOINT.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30a4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "AST_MODEL = 'MIT/ast-finetuned-audioset-10-10-0.4593'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE_AST = 2\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "VAL_FRACTION = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22599db",
   "metadata": {},
   "source": [
    "\n",
    "## Build finetune manifest\n",
    "Uses `scripts/build_finetune_dataset.py`: per-class F1 winner on non-IRR gold; inner-join teachers; writes CSV/Parquet/HF dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec55130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied consensus voting with threshold 2 over teachers: ['ast', 'clap', 'm2d', 'whisper']\n",
      "Teacher row counts: {'ast': 6196, 'clap': 6196, 'm2d': 6196, 'whisper': 6196}\n",
      "Segment intersection size across teachers: 6196\n",
      "Built merged dataset with 6196 rows and 34 columns\n",
      "Wrote Parquet to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune.parquet\n",
      "Wrote CSV to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5152ef5fde2646dcb6cf39a0733d95bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF dataset to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scripts.build_finetune_dataset import build_dataset\n",
    "\n",
    "out_disk = METADATA_DIR / 'blocs_smad_v2_finetune'\n",
    "out_parquet = METADATA_DIR / 'blocs_smad_v2_finetune.parquet'\n",
    "out_csv = MANIFEST\n",
    "\n",
    "build_dataset(METADATA_DIR, out_disk, out_parquet, out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7013cf2",
   "metadata": {},
   "source": [
    "\n",
    "## Validate manifest\n",
    "Checks for dupes, required columns, chosen_* nulls, and optional gold sanity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9db4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest: data/metadata/blocs_smad_v2_finetune.csv rows=6196 columns=34\n",
      "No duplicate segment_path entries.\n",
      "All required teacher and chosen columns present.\n",
      "Chosen columns have no nulls.\n",
      "Value counts for chosen_speech_label: {1: 5715, 0: 481}\n",
      "Value counts for chosen_music_label: {0: 5510, 1: 686}\n",
      "Value counts for chosen_noise_label: {0: 6182, 1: 14}\n",
      "Merged IRR gold rows: 174\n",
      "\n",
      "IRR gold sanity for speech:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8293    1.0000    0.9067        34\n",
      "           1     1.0000    0.9500    0.9744       140\n",
      "\n",
      "    accuracy                         0.9598       174\n",
      "   macro avg     0.9146    0.9750    0.9405       174\n",
      "weighted avg     0.9666    0.9598    0.9611       174\n",
      "\n",
      "\n",
      "IRR gold sanity for music:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9746    1.0000    0.9871       115\n",
      "           1     1.0000    0.9492    0.9739        59\n",
      "\n",
      "    accuracy                         0.9828       174\n",
      "   macro avg     0.9873    0.9746    0.9805       174\n",
      "weighted avg     0.9832    0.9828    0.9826       174\n",
      "\n",
      "\n",
      "IRR gold sanity for noise:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8736    1.0000    0.9325       152\n",
      "           1     0.0000    0.0000    0.0000        22\n",
      "\n",
      "    accuracy                         0.8736       174\n",
      "   macro avg     0.4368    0.5000    0.4663       174\n",
      "weighted avg     0.7631    0.8736    0.8146       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scripts.validate_finetune_manifest import main as validate_main\n",
    "validate_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab18d75",
   "metadata": {},
   "source": [
    "\n",
    "## Train AST student\n",
    "Fine-tune AST with BCEWithLogits, class pos_weight, train/val split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29438458",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluate on gold\n",
    "Default filter is IRR; adjust `gold_filter` or `threshold` as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54312783",
   "metadata": {},
   "source": [
    "\n",
    "## Holdout Gold Eval (non-IRR)\n",
    "We reserve a gold, non-IRR subset for evaluation and drop it from training. Teacher metrics and the student are compared on this holdout set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a363fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold non-IRR total: 1569 | holdout: 314 | remaining for calibration: 1255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from pathlib import Path\n",
    "\n",
    "CLASSES = [\"speech\", \"music\", \"noise\"]\n",
    "\n",
    "# Load gold labels and pick non-IRR rows\n",
    "GOLD_PATH = METADATA_DIR / \"blocs_smad_gold_annotations_v1.csv\"\n",
    "gold_df = pd.read_csv(GOLD_PATH)\n",
    "gold_non_irr = gold_df[gold_df[\"is_irr_segment\"] == False].copy().reset_index(drop=True)\n",
    "\n",
    "# Sample a holdout set (20% by default)\n",
    "HOLDOUT_FRAC = 0.2\n",
    "RNG_SEED = 42\n",
    "holdout_df = gold_non_irr.sample(frac=HOLDOUT_FRAC, random_state=RNG_SEED)\n",
    "train_gold_df = gold_non_irr.drop(holdout_df.index)\n",
    "\n",
    "print(f\"Gold non-IRR total: {len(gold_non_irr)} | holdout: {len(holdout_df)} | remaining for calibration: {len(train_gold_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29445ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training manifest rows after dropping holdout: 5882 (dropped 314)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reload finetune manifest and drop holdout segments for training\n",
    "manifest_df = pd.read_csv(MANIFEST)\n",
    "holdout_paths = set(holdout_df[\"segment_path\"])\n",
    "train_manifest = manifest_df[~manifest_df[\"segment_path\"].isin(holdout_paths)].reset_index(drop=True)\n",
    "print(f\"Training manifest rows after dropping holdout: {len(train_manifest)} (dropped {len(manifest_df)-len(train_manifest)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3617c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote training manifest (holdout removed) to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune_train.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_MANIFEST = METADATA_DIR / 'blocs_smad_v2_finetune_train.csv'\n",
    "train_manifest.to_csv(TRAIN_MANIFEST, index=False)\n",
    "print(f\"Wrote training manifest (holdout removed) to {TRAIN_MANIFEST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d285347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Compute stats for train/holdout/total and gold overlap\n",
    "manifest_df = pd.read_csv(MANIFEST)\n",
    "train_df = pd.read_csv(TRAIN_MANIFEST)\n",
    "holdout_paths = set(manifest_df[\"segment_path\"]) - set(train_df[\"segment_path\"])\n",
    "holdout_df = manifest_df[manifest_df[\"segment_path\"].isin(holdout_paths)].reset_index(drop=True)\n",
    "\n",
    "# Derive segment duration from one file\n",
    "sample_path = SEGMENTS_DIR / manifest_df.iloc[0][\"segment_path\"]\n",
    "wav, sr = torchaudio.load(sample_path)\n",
    "duration_sec = wav.shape[-1] / sr\n",
    "\n",
    "# Load gold annotations to count gold segments in each split\n",
    "gold_df = pd.read_csv(GOLD)\n",
    "\n",
    "def stats(df, name):\n",
    "    n = len(df)\n",
    "    hours = n * duration_sec / 3600.0\n",
    "    speech_pct = float(df[\"chosen_speech_label\"].mean() * 100)\n",
    "    music_pct = float(df[\"chosen_music_label\"].mean() * 100)\n",
    "    noise_pct = float(df[\"chosen_noise_label\"].mean() * 100)\n",
    "    gold_count = gold_df[gold_df[\"segment_path\"].isin(df[\"segment_path\"])].shape[0]\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"segments\": n,\n",
    "        \"hours\": hours,\n",
    "        \"%speech\": speech_pct,\n",
    "        \"%music\": music_pct,\n",
    "        \"%noise\": noise_pct,\n",
    "        \"gold_segs\": gold_count,\n",
    "    }\n",
    "\n",
    "rows = [\n",
    "    stats(train_df, \"train\"),\n",
    "    stats(holdout_df, \"holdout\"),\n",
    "    stats(manifest_df, \"total\"),\n",
    "]\n",
    "\n",
    "stats_df = pd.DataFrame(rows)\n",
    "stats_df[\"hours\"] = stats_df[\"hours\"].map(lambda h: round(h, 2))\n",
    "for col in [\"%speech\", \"%music\", \"%noise\"]:\n",
    "    stats_df[col] = stats_df[col].map(lambda x: round(x, 2))\n",
    "stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266bc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen encoder; training head only (3,843/86,191,107 parameters).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3e3dbd18664767b29a51e503b620f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 1/10:   0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_student\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m      4\u001b[39m train_args = argparse.Namespace(\n\u001b[32m      5\u001b[39m     manifest=TRAIN_MANIFEST,\n\u001b[32m      6\u001b[39m     segments_dir=SEGMENTS_DIR,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     ast_model=AST_MODEL,\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/scripts/train_student.py:122\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    120\u001b[39m     loss.backward()\n\u001b[32m    121\u001b[39m     optim.step()\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * input_values.size(\u001b[32m0\u001b[39m)\n\u001b[32m    123\u001b[39m     train_pbar.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m epoch_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "from scripts.train_student import train\n",
    "\n",
    "train_args = argparse.Namespace(\n",
    "    manifest=TRAIN_MANIFEST,\n",
    "    segments_dir=SEGMENTS_DIR,\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    hop_length=160,\n",
    "    win_length=400,\n",
    "    batch_size_ast=BATCH_SIZE_AST,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    val_fraction=VAL_FRACTION,\n",
    "    output=CHECKPOINT,\n",
    "    ast_model=AST_MODEL,\n",
    ")\n",
    "train(train_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: compute per-class precision/recall/F1 given gold + prediction columns\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    macro_f1 = float(f1.mean())\n",
    "    per_class = {c: {\"precision\": float(p), \"recall\": float(r), \"f1\": float(f)} for c, p, r, f in zip(CLASSES, prec, rec, f1)}\n",
    "    return per_class, macro_f1\n",
    "\n",
    "# Build gold matrix for holdout\n",
    "holdout_gold = holdout_df[[f\"{c}_gold\" for c in CLASSES]].to_numpy()\n",
    "\n",
    "# Join holdout with teacher predictions\n",
    "joined = holdout_df.merge(manifest_df, on=\"segment_path\", how=\"left\", suffixes=(\"_gold\", \"\"))\n",
    "missing = joined[\"segment_path\"].isna().sum()\n",
    "if missing:\n",
    "    print(f\"Warning: {missing} holdout segments missing from manifest (will drop)\")\n",
    "    joined = joined.dropna(subset=[\"segment_path\"]).reset_index(drop=True)\n",
    "\n",
    "teacher_sources = {\n",
    "    \"ast\": [f\"ast_{c}_label\" for c in CLASSES],\n",
    "    \"whisper\": [f\"whisper_{c}_label\" for c in CLASSES],\n",
    "    \"clap\": [f\"clap_{c}_label\" for c in CLASSES],\n",
    "    \"m2d\": [f\"m2d_{c}_label\" for c in CLASSES],\n",
    "    \"consensus\": [f\"chosen_{c}_label\" for c in CLASSES],\n",
    "}\n",
    "\n",
    "metrics_rows = []\n",
    "for name, cols in teacher_sources.items():\n",
    "    if not set(cols).issubset(joined.columns):\n",
    "        continue\n",
    "    y_pred = joined[cols].to_numpy()\n",
    "    per_class, macro_f1 = compute_metrics(holdout_gold, y_pred)\n",
    "    metrics_rows.append({\"model\": name, \"macro_f1\": macro_f1, **{f\"f1_{c}\": per_class[c][\"f1\"] for c in CLASSES}})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd25f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot per-class F1 for teachers/consensus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if not metrics_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    x = np.arange(len(metrics_df))\n",
    "    width = 0.2\n",
    "    for i, c in enumerate(CLASSES):\n",
    "        ax.bar(x + i*width, metrics_df[f\"f1_{c}\"], width, label=c)\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(metrics_df[\"model\"], rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"F1 (holdout)\")\n",
    "    ax.set_title(\"Teacher/Consensus F1 on gold holdout\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No metrics to plot (metrics_df empty)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#evaluate trained student on holdout (requires checkpoint_path)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from finetune.ast_model import ASTClassifier, get_feature_extractor\n",
    "from finetune.dataset import SmadDataset\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(\"checkpoints/student_ast.pt\")\n",
    "if checkpoint_path.exists():\n",
    "    device = torch.device(\"mps\" if getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    state = torch.load(checkpoint_path, map_location=device)\n",
    "    student = ASTClassifier(model_name=AST_MODEL, num_labels=3).to(device)\n",
    "    student.load_state_dict(state[\"model_state_dict\"], strict=False)\n",
    "    student.eval()\n",
    "    processor = get_feature_extractor(AST_MODEL)\n",
    "\n",
    "    holdout_ds = SmadDataset(\n",
    "        manifest_path=MANIFEST,\n",
    "        segments_dir=SEGMENTS_DIR,\n",
    "        sample_rate=16000,\n",
    "        n_mels=128,\n",
    "        hop_length=160,\n",
    "        win_length=400,\n",
    "        return_waveform=True,\n",
    "    )\n",
    "    # filter to holdout paths\n",
    "    holdout_indices = [i for i, p in enumerate(holdout_ds.df[\"segment_path\"]) if p in holdout_paths]\n",
    "    holdout_subset = torch.utils.data.Subset(holdout_ds, holdout_indices)\n",
    "    loader = DataLoader(holdout_subset, batch_size=4, shuffle=False)\n",
    "\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            wavs = batch[\"waveform\"].squeeze(1)\n",
    "            wav_list = [w.cpu().numpy() for w in wavs]\n",
    "            inputs = processor(wav_list, sampling_rate=processor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "            input_values = inputs[\"input_values\"].to(device)\n",
    "            attention_mask = inputs.get(\"attention_mask\")\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(device)\n",
    "            logits = student(input_values=input_values, attention_mask=attention_mask)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    logits = torch.cat(all_logits)\n",
    "    labels = torch.cat(all_labels)\n",
    "    probs = torch.sigmoid(logits).numpy()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    student_per_class, student_macro = compute_metrics(labels.numpy(), preds)\n",
    "    print(\"Student macro F1 on holdout:\", student_macro)\n",
    "else:\n",
    "    print(f\"No checkpoint found at {checkpoint_path}; skip student eval.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
