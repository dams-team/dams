{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c832df5",
   "metadata": {},
   "source": [
    "\n",
    "# Finetune SMAD Student (AST) End-to-End\n",
    "\n",
    "This notebook builds the fused pseudo-label manifest, validates it, trains the AST student, and evaluates on gold.\n",
    "\n",
    "**Assumptions**\n",
    "- Run from within the repo; audio segments live in `data/segments/`.\n",
    "- Dependencies installed: torch, torchaudio, transformers, pandas, datasets, scikit-learn.\n",
    "- Teachers HF datasets are under `data/metadata/blocs_smad_v2_*`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121b9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /Users/benji/Desktop/columbia/dams\n",
      "Working dir set to: /Users/benji/Desktop/columbia/dams\n"
     ]
    }
   ],
   "source": [
    "# Resolve project root and set paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / 'data').exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "import os\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f'Working dir set to: {Path.cwd()}')\n",
    "\n",
    "METADATA_DIR = PROJECT_ROOT / 'data/metadata'\n",
    "SEGMENTS_DIR = PROJECT_ROOT / 'data/segments'\n",
    "MANIFEST = METADATA_DIR / 'blocs_smad_v2_finetune.csv'\n",
    "GOLD = METADATA_DIR / 'blocs_smad_gold_annotations_v1.csv'\n",
    "CHECKPOINT = PROJECT_ROOT / 'checkpoints/student_ast.ipynb_run.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30a4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "AST_MODEL = 'MIT/ast-finetuned-audioset-10-10-0.4593'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE_AST = 2\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "VAL_FRACTION = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22599db",
   "metadata": {},
   "source": [
    "\n",
    "## Build finetune manifest\n",
    "Uses `scripts/build_finetune_dataset.py`: per-class F1 winner on non-IRR gold; inner-join teachers; writes CSV/Parquet/HF dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec55130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using non-IRR calibration subset with 1569 rows\n",
      "Best speech teacher: m2d (F1 0.9592)\n",
      "Best music teacher: ast (F1 0.9768)\n",
      "Best noise teacher: clap (F1 0.0391)\n",
      "Teacher row counts: {'ast': 6196, 'clap': 6196, 'm2d': 6196, 'whisper': 6196}\n",
      "Segment intersection size across teachers: 6196\n",
      "Built merged dataset with 6196 rows and 34 columns\n",
      "Wrote Parquet to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune.parquet\n",
      "Wrote CSV to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a483477798414548b79d272fafc19268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF dataset to /Users/benji/Desktop/columbia/dams/data/metadata/blocs_smad_v2_finetune\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scripts.build_finetune_dataset import build_dataset\n",
    "\n",
    "out_disk = METADATA_DIR / 'blocs_smad_v2_finetune'\n",
    "out_parquet = METADATA_DIR / 'blocs_smad_v2_finetune.parquet'\n",
    "out_csv = MANIFEST\n",
    "\n",
    "build_dataset(METADATA_DIR, out_disk, out_parquet, out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7013cf2",
   "metadata": {},
   "source": [
    "\n",
    "## Validate manifest\n",
    "Checks for dupes, required columns, chosen_* nulls, and optional gold sanity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9db4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest: data/metadata/blocs_smad_v2_finetune.csv rows=6196 columns=34\n",
      "No duplicate segment_path entries.\n",
      "All required teacher and chosen columns present.\n",
      "Chosen columns have no nulls.\n",
      "Value counts for chosen_speech_label: {1: 5698, 0: 498}\n",
      "Value counts for chosen_music_label: {0: 5488, 1: 708}\n",
      "Value counts for chosen_noise_label: {0: 6178, 1: 18}\n",
      "Merged IRR gold rows: 174\n",
      "\n",
      "IRR gold sanity for speech:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8718    1.0000    0.9315        34\n",
      "           1     1.0000    0.9643    0.9818       140\n",
      "\n",
      "    accuracy                         0.9713       174\n",
      "   macro avg     0.9359    0.9821    0.9567       174\n",
      "weighted avg     0.9749    0.9713    0.9720       174\n",
      "\n",
      "\n",
      "IRR gold sanity for music:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9746    1.0000    0.9871       115\n",
      "           1     1.0000    0.9492    0.9739        59\n",
      "\n",
      "    accuracy                         0.9828       174\n",
      "   macro avg     0.9873    0.9746    0.9805       174\n",
      "weighted avg     0.9832    0.9828    0.9826       174\n",
      "\n",
      "\n",
      "IRR gold sanity for noise:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8728    0.9934    0.9292       152\n",
      "           1     0.0000    0.0000    0.0000        22\n",
      "\n",
      "    accuracy                         0.8678       174\n",
      "   macro avg     0.4364    0.4967    0.4646       174\n",
      "weighted avg     0.7625    0.8678    0.8117       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scripts.validate_finetune_manifest import main as validate_main\n",
    "validate_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab18d75",
   "metadata": {},
   "source": [
    "\n",
    "## Train AST student\n",
    "Fine-tune AST with BCEWithLogits, class pos_weight, train/val split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266bc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen encoder; training head only (3,843/86,191,107 parameters).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462cbc494b07429e842471433f661d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train 1/10:   0%|          | 0/2789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_student\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m      4\u001b[39m train_args = argparse.Namespace(\n\u001b[32m      5\u001b[39m     manifest=MANIFEST,\n\u001b[32m      6\u001b[39m     segments_dir=SEGMENTS_DIR,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     ast_model=AST_MODEL,\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/columbia/dams/scripts/train_student.py:122\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    120\u001b[39m     loss.backward()\n\u001b[32m    121\u001b[39m     optim.step()\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * input_values.size(\u001b[32m0\u001b[39m)\n\u001b[32m    123\u001b[39m     train_pbar.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m epoch_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "from scripts.train_student import train\n",
    "\n",
    "train_args = argparse.Namespace(\n",
    "    manifest=MANIFEST,\n",
    "    segments_dir=SEGMENTS_DIR,\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    hop_length=160,\n",
    "    win_length=400,\n",
    "    batch_size_ast=BATCH_SIZE_AST,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    val_fraction=VAL_FRACTION,\n",
    "    output=CHECKPOINT,\n",
    "    ast_model=AST_MODEL,\n",
    ")\n",
    "train(train_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29438458",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluate on gold\n",
    "Default filter is IRR; adjust `gold_filter` or `threshold` as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "from scripts.evaluate_student import evaluate\n",
    "\n",
    "eval_args = argparse.Namespace(\n",
    "    checkpoint=CHECKPOINT,\n",
    "    manifest=MANIFEST,\n",
    "    segments_dir=SEGMENTS_DIR,\n",
    "    gold=GOLD,\n",
    "    gold_filter='irr',  # options: 'irr', 'non-irr', 'all'\n",
    "    batch_size=2,\n",
    "    threshold=0.5,\n",
    "    sample_rate=16000,\n",
    "    ast_model=AST_MODEL,\n",
    ")\n",
    "evaluate(eval_args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
