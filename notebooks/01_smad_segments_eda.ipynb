{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 01 - BLOCS SMAD Segments EDA\n",
    "\n",
    "Exploratory data analysis for the BLOCS segment manifest and the acoustic stats / QC flags produced by `build_acoustic_stats.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "\n",
    "This notebook assumes you have:\n",
    "\n",
    "1. Synced audio and segments locally with `python scripts/sync_b2_data.py`.\n",
    "2. Built the acoustic-stats manifest with `python -m data_processing.build_acoustic_stats`.\n",
    "\n",
    "If you want to experiment freely, make a personal copy in `notebooks/local` (e.g., `01_smad_segments_eda_<yourname>.ipynb`) and work in that file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load configuration and datasets\n",
    "\n",
    "In this section we load project settings, point to the BLOCS metadata directory, and open the base segment manifest (`blocs_smad_segments`) and the acoustic-stats–augmented manifest (`blocs_smad_v1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from utils.config_utils import load_env, add_project_root_to_path\n",
    "load_env()\n",
    "\n",
    "from config import get_settings\n",
    "from utils.dams_types import BLOCS_SMAD_SEGMENTS, BLOCS_SMAD_V1\n",
    "\n",
    "add_project_root_to_path()\n",
    "# Load project settings for file paths and parameters.\n",
    "settings = get_settings()\n",
    "metadata_dir = Path(settings.metadata_path)\n",
    "segments_dir = Path(settings.segments_path)\n",
    "\n",
    "# Define manifest paths.\n",
    "base_manifest_path = metadata_dir / BLOCS_SMAD_SEGMENTS\n",
    "acoustic_manifest_path = metadata_dir / BLOCS_SMAD_V1\n",
    "\n",
    "# Load HF datasets to pandas dataframes.\n",
    "ds_segments = load_from_disk(base_manifest_path)\n",
    "ds_acoustic = load_from_disk(acoustic_manifest_path)\n",
    "df_segments = ds_segments.to_pandas()\n",
    "df_acoustic = ds_acoustic.to_pandas()\n",
    "\n",
    "len(df_segments), len(df_acoustic) # Should be the same number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 1.1 HuggingFace Datasets\n",
    "\n",
    "This project uses HuggingFace Datasets. If unfamiliar, you may want to check out their tutorials: https://huggingface.co/docs/datasets/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Inspect schema and basic statistics\n",
    "\n",
    "Here we inspect columns, dtypes, and simple descriptive statistics for segment-level metadata.\n",
    "The goal is to get a feel for what a “segment row” looks like before training.\n",
    "\n",
    "+ `split`: train/val/test split designation. Default is unsplit.\n",
    "+ `label_source`: source of transcription labels (none, gold, or specific model). Default is none.\n",
    "+ `*_label`: binary multi-label targets for presence of speech, music, or noise (0/1).\n",
    "+ `*_score`: teacher confidence scores for specific label types (0.0–1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few rows of segment-level metadata from blocs_smad_segments.\n",
    "\n",
    "display(df_segments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few rows of acoustic stats and QC flags from blocs_smad_v1.\n",
    "\n",
    "display(df_acoustic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get familiar with the acoustic names and dtypes.\n",
    "\n",
    "df_acoustic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics for numeric columns.\n",
    "\n",
    "with pd.option_context('display.float_format', '{:.6f}'.format):\n",
    "    display(df_acoustic.describe(include=\"number\").T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 3. Acoustic flags: prevalence and distribution\n",
    "\n",
    "In this section we summarize how often each acoustic flag fires (too quiet, mostly silence, heavily clipped, too short, had error), both as counts and percentages. This helps sanity-check the thresholds used in `build_acoustic_stats.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize prevalence of acoustic QC flags.\n",
    "\n",
    "# Identify all boolean or 0/1 flag columns.\n",
    "flag_cols = [\n",
    "    col for col in df_acoustic.columns\n",
    "    if col.endswith('_flag') or col in\n",
    "       ['too_quiet', 'mostly_silence', 'heavily_clipped',\n",
    "        'too_short', 'had_error']\n",
    "    if col in df_acoustic.columns\n",
    "]\n",
    "\n",
    "# Compute counts and percentages.\n",
    "flag_summary = pd.DataFrame({\n",
    "    'count': df_acoustic[flag_cols].sum(),\n",
    "    'percent': (df_acoustic[flag_cols].mean() * 100)\n",
    "}).sort_values('percent', ascending=False)\n",
    "\n",
    "with pd.option_context('display.float_format', '{:.2f}'.format):\n",
    "    display(flag_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 4. Distributions of core acoustic features\n",
    "\n",
    "We visualize the distributions of key numeric features (for example duration, RMS energy, peak amplitude) and look for outliers or pathological patterns that might affect training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def set_plot_style() -> None:\n",
    "    \"\"\"Set a clean, readable style for EDA plots without grid lines.\"\"\"\n",
    "    sns.set_style(\"white\", {\"axes.grid\": False})\n",
    "    sns.set_context(\"talk\")\n",
    "\n",
    "\n",
    "def style_generic_plot(ax, title: str, xlabel: str, ylabel: str) -> None:\n",
    "    \"\"\"Apply consistent styling to a single axis.\"\"\"\n",
    "    sns.despine(ax=ax)  # removes top and right spines\n",
    "\n",
    "    # turn OFF all gridlines\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "\n",
    "set_plot_style()\n",
    "\n",
    "# Core acoustic features we care about for QC and modeling.\n",
    "acoustic_cols = [\n",
    "    \"rms_db\",\n",
    "    \"silence_ratio\",\n",
    "    \"zero_crossing_rate\",\n",
    "    \"snr_db\",\n",
    "    \"energy_variance\",\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the current manifest.\n",
    "acoustic_cols = [c for c in acoustic_cols if c in df_acoustic.columns]\n",
    "\n",
    "# HUSL palette\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(acoustic_cols))\n",
    "\n",
    "fig, axes = plt.subplots(len(acoustic_cols), 1, figsize=(8, 2.5 * len(acoustic_cols)))\n",
    "\n",
    "if len(acoustic_cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, col, color in zip(axes, acoustic_cols, colors):\n",
    "    ax.hist(df_acoustic[col], bins=40, color=color, edgecolor=\"black\", alpha=0.9)\n",
    "    style_generic_plot(\n",
    "        ax,\n",
    "        title=f\"Distribution of {col}\",\n",
    "        xlabel=col,\n",
    "        ylabel=\"count\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Listen to example segments\n",
    "\n",
    "To ground the numbers in actual audio, we sample a few segments and listen to them. We can filter by QC flags (for example “good” segments versus “mostly silence”) to verify that the flags match our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Audio, display\n",
    "\n",
    "from utils.audio_io import load_waveform\n",
    "\n",
    "\n",
    "def _resolve_segment_path(row) -> Path:\n",
    "    \"\"\"Resolve the on-disk path for a segment from a row.\"\"\"\n",
    "    candidate_cols = [\"segment_path\", \"segment_relpath\", \"segment_file\"]\n",
    "    for col in candidate_cols:\n",
    "        if col in row.index:\n",
    "            return segments_dir / row[col]\n",
    "    raise KeyError(\n",
    "        \"Could not find a segment path column. \"\n",
    "        \"Update `_resolve_segment_path` with the correct column name.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def play_random_segment(df, query: str | None = None, sample_rate: int = 16_000) -> None:\n",
    "    \"\"\"Sample a random segment (optionally filtered by a query) and play it.\"\"\"\n",
    "    subset = df.query(query) if query else df\n",
    "    if subset.empty:\n",
    "        print(\"No rows matched the query.\")\n",
    "        return\n",
    "\n",
    "    row = subset.sample(1).iloc[0]\n",
    "    audio_path = _resolve_segment_path(row)\n",
    "\n",
    "    # Clean filename for display\n",
    "    segment_name = audio_path.name\n",
    "\n",
    "    print(f\"\\n▶ Segment: {segment_name}\")\n",
    "    print(f\"   Path: {audio_path}\")\n",
    "    print()\n",
    "\n",
    "    waveform, sr = load_waveform(audio_path)\n",
    "    sr = sr or sample_rate\n",
    "\n",
    "    # Show the row metadata and play the audio.\n",
    "    display(row)\n",
    "    display(Audio(waveform.squeeze().numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define a function to play a grid of segments.\n",
    "\n",
    "def play_segment_grid(\n",
    "    df,\n",
    "    query: str | None = None,\n",
    "    n: int = 6,\n",
    "    n_cols: int = 3,\n",
    "    sample_rate: int = 16_000,\n",
    ") -> None:\n",
    "    \"\"\"Display a small grid of audio segments with inline players.\"\"\"\n",
    "    subset = df.query(query) if query else df\n",
    "    if subset.empty:\n",
    "        print(\"No rows matched the query.\")\n",
    "        return\n",
    "\n",
    "    n = min(n, len(subset))\n",
    "    sampled = subset.sample(n, random_state=0)\n",
    "\n",
    "    cell_html_blocks: list[str] = []\n",
    "\n",
    "    for _, row in sampled.iterrows():\n",
    "        audio_path = _resolve_segment_path(row)\n",
    "        waveform, sr = load_waveform(audio_path)\n",
    "        sr = sr or sample_rate\n",
    "\n",
    "        segment_name = audio_path.name\n",
    "\n",
    "        # Build an embedded audio player for this segment\n",
    "        audio_widget = Audio(waveform.squeeze().numpy(), rate=sr, embed=True)\n",
    "        audio_html = audio_widget._repr_html_()\n",
    "\n",
    "        # Wrap filename + audio in a small card\n",
    "        card_html = f\"\"\"\n",
    "        <div style=\"font-size:12px; margin-bottom:4px; font-family:monospace;\">\n",
    "            {segment_name}\n",
    "        </div>\n",
    "        {audio_html}\n",
    "        \"\"\"\n",
    "        cell_html_blocks.append(card_html)\n",
    "\n",
    "    # Arrange cards into a table grid\n",
    "    rows_html: list[str] = []\n",
    "    for i in range(0, len(cell_html_blocks), n_cols):\n",
    "        cells = cell_html_blocks[i : i + n_cols]\n",
    "        row_html = \"\".join(\n",
    "            f\"<td style='padding:8px; vertical-align:top;'>{cell}</td>\"\n",
    "            for cell in cells\n",
    "        )\n",
    "        rows_html.append(f\"<tr>{row_html}</tr>\")\n",
    "\n",
    "    table_html = \"<table><tbody>\" + \"\".join(rows_html) + \"</tbody></table>\"\n",
    "    display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Completely random segment\n",
    "play_random_segment(df_acoustic)\n",
    "\n",
    "# 2) A \"good\" segment that passed all QC flags (uncomment if these columns exist)\n",
    "# play_random_segment(\n",
    "#     df_acoustic,\n",
    "#     query=\"(~too_quiet) and (~mostly_silence) and (~heavily_clipped) and (~too_short) and (~had_error)\",\n",
    "# )\n",
    "\n",
    "# 3) A segment that is mostly silence (uncomment if `mostly_silence` exists)\n",
    "# play_random_segment(df_acoustic, query=\"mostly_silence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 random segments from the whole distribution\n",
    "play_segment_grid(df_acoustic, n=6, n_cols=3)\n",
    "\n",
    "# 6 segments that are mostly silence (if that flag exists)\n",
    "# play_segment_grid(df_acoustic, query=\"mostly_silence\", n=6, n_cols=3)\n",
    "\n",
    "# 6 \"good\" segments with no QC issues\n",
    "# play_segment_grid(\n",
    "#     df_acoustic,\n",
    "#     query=\"(~too_quiet) and (~mostly_silence) and (~heavily_clipped) and (~too_short) and (~had_error)\",\n",
    "#     n=6,\n",
    "#     n_cols=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 6. Correlations among acoustic features\n",
    "\n",
    "To understand how the core acoustic statistics relate to each other and whether they form meaningful structure for modeling, we compute correlation matrices and inspect which features co-vary. This helps us see whether the dataset naturally separates into regimes (for example speech-like, music-like, noise-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric acoustic features.\n",
    "\n",
    "numeric_cols = df_acoustic.select_dtypes(include=\"number\").columns\n",
    "exclude_cols = [\n",
    "    \"speech_label\", \"music_label\", \"noise_label\",\n",
    "    \"speech_score\", \"music_score\", \"noise_score\",\n",
    "    \"start_time\", \"end_time\",\n",
    "]\n",
    "numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "\n",
    "corr = df_acoustic[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(\n",
    "    corr,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0.0,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    xticklabels=numeric_cols,\n",
    "    yticklabels=numeric_cols,\n",
    "    linewidths=0.5,       # <-- add this\n",
    "    linecolor=\"white\",    # <-- and this\n",
    "    vmin=-1.0,\n",
    "    vmax=1.0,\n",
    ")\n",
    "\n",
    "# Title and axis label styling\n",
    "ax.set_title(\n",
    "    \"Correlation matrix of acoustic features\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Tilt x-axis labels so they are readable.\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    ha=\"right\",\n",
    "    fontsize=10,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "ax.set_yticklabels(\n",
    "    ax.get_yticklabels(),\n",
    "    rotation=0,\n",
    "    fontsize=10,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 7. Distribution of gold labels (speech, music, noise)\n",
    "\n",
    "This section activates once gold labels are mapped onto segments. For now, it will print a message if no label_source == \"gold\" rows exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check per class distributions of gold labels.\n",
    "\n",
    "label_cols = ['speech_label', 'music_label', 'noise_label']\n",
    "\n",
    "df_gold = df_acoustic[df_acoustic['label_source'] == 'gold'].copy()\n",
    "\n",
    "if df_gold.empty:\n",
    "    print(\"No gold labels found yet (label_source == 'gold'). \"\n",
    "          \"Run the gold-to-segment mapping pipeline first.\")\n",
    "else:\n",
    "    # Per-class prevalence\n",
    "    print(\"Per-class prevalence among gold labels:\")\n",
    "    display(df_gold[label_cols].mean().to_frame('prevalence'))\n",
    "\n",
    "    # Multi-label combo distribution\n",
    "    combo = (\n",
    "        df_gold[label_cols]\n",
    "        .astype(int)\n",
    "        .astype(str)\n",
    "        .agg(''.join, axis=1)\n",
    "    )\n",
    "    print(\"\\nMulti-label pattern distribution (e.g., 100 = speech only):\")\n",
    "    display(combo.value_counts(normalize=True).to_frame('fraction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. Pairwise relationships between acoustic features\n",
    "\n",
    "To capture simple nonlinear structure that correlations might miss, we look at selected 2D projections of the acoustic feature space (for example `rms_db` vs `snr_db`). This helps reveal distinct regimes (for example quiet speech, loud music) and potential overlapping regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected 2D acoustic feature relationships with stable hexbin plots.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "pair_specs = [\n",
    "    (\"rms_db\", \"snr_db\"),\n",
    "    (\"rms_db\", \"silence_ratio\"),\n",
    "    (\"zero_crossing_rate\", \"rms_db\"),\n",
    "]\n",
    "\n",
    "# Collect all columns used in the pairs.\n",
    "subset_cols = {c for pair in pair_specs for c in pair}\n",
    "subset_cols = [c for c in subset_cols if c in df_acoustic.columns]\n",
    "\n",
    "pair_sample = df_acoustic[subset_cols].copy()\n",
    "\n",
    "# Replace infinities with NaN, then drop any NaNs.\n",
    "pair_sample = pair_sample.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Subsample for readability if it is huge.\n",
    "if len(pair_sample) > 20000:\n",
    "    pair_sample = pair_sample.sample(20000, random_state=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(pair_specs), figsize=(5 * len(pair_specs), 4))\n",
    "\n",
    "if len(pair_specs) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (x_col, y_col) in zip(axes, pair_specs):\n",
    "    if x_col not in pair_sample.columns or y_col not in pair_sample.columns:\n",
    "        continue\n",
    "\n",
    "    hb = ax.hexbin(\n",
    "        pair_sample[x_col],\n",
    "        pair_sample[y_col],\n",
    "        gridsize=40,\n",
    "        cmap=\"mako\",\n",
    "        mincnt=1,\n",
    "    )\n",
    "    style_generic_plot(\n",
    "        ax,\n",
    "        title=f\"{y_col} vs {x_col}\",\n",
    "        xlabel=x_col,\n",
    "        ylabel=y_col,\n",
    "    )\n",
    "    # Optional: colorbar per subplot\n",
    "    cb = fig.colorbar(hb, ax=ax, shrink=0.75)\n",
    "    cb.set_label(\"count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 9. PCA projection of acoustic features\n",
    "\n",
    "We project the standardized acoustic features into a low-dimensional space using PCA. This provides a coarse view of how segments are arranged in feature space and whether there are obvious clusters or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_scatter(X_pca, explained_var):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Dark background to match your acoustic plots\n",
    "    ax.set_facecolor('#1e1e1e')\n",
    "    fig.patch.set_facecolor('#1e1e1e')\n",
    "\n",
    "    # Single scatter color to match previous figures\n",
    "    point_color = '#4db8ff'\n",
    "\n",
    "    ax.scatter(\n",
    "        X_pca[:, 0],\n",
    "        X_pca[:, 1],\n",
    "        s=10,\n",
    "        alpha=0.35,\n",
    "        color=point_color,\n",
    "        edgecolors='none'\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        \"PCA of acoustic features\",\n",
    "        fontsize=18,\n",
    "        color='white',\n",
    "        pad=12\n",
    "    )\n",
    "    ax.set_xlabel(\n",
    "        f\"PC1 ({explained_var[0]:.1f} percent var)\",\n",
    "        fontsize=14,\n",
    "        color='white'\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        f\"PC2 ({explained_var[1]:.1f} percent var)\",\n",
    "        fontsize=14,\n",
    "        color='white'\n",
    "    )\n",
    "\n",
    "    ax.tick_params(axis='both', colors='white', labelsize=12)\n",
    "\n",
    "    # White spines to match your correlation heatmap and hexbin style\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('white')\n",
    "        spine.set_linewidth(1.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Reuse the core acoustic columns from earlier.\n",
    "pca_features = [\n",
    "    \"rms_db\",\n",
    "    \"silence_ratio\",\n",
    "    \"zero_crossing_rate\",\n",
    "    \"snr_db\",\n",
    "    \"energy_variance\",\n",
    "]\n",
    "pca_features = [c for c in pca_features if c in df_acoustic.columns]\n",
    "\n",
    "X = df_acoustic[pca_features].copy()\n",
    "\n",
    "# 1) Clean: remove inf / -inf / NaN.\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# 2) Drop constant columns.\n",
    "zero_var_cols = [c for c in X.columns if X[c].nunique() <= 1]\n",
    "if zero_var_cols:\n",
    "    print(\"Dropping zero-variance columns for PCA:\", zero_var_cols)\n",
    "    X = X.drop(columns=zero_var_cols)\n",
    "\n",
    "if X.shape[1] < 2:\n",
    "    raise ValueError(f\"Need at least 2 non-constant numeric features, got {X.shape[1]}\")\n",
    "\n",
    "# 3) Standardize.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = np.asarray(X_scaled, dtype=np.float64)\n",
    "\n",
    "# Final sanity check on the scaled matrix.\n",
    "finite_mask = np.isfinite(X_scaled).all(axis=1)\n",
    "if not finite_mask.all():\n",
    "    dropped = (~finite_mask).sum()\n",
    "    print(f\"Dropping {dropped} rows with non-finite values after scaling.\")\n",
    "    X_scaled = X_scaled[finite_mask]\n",
    "    X = X.iloc[finite_mask].reset_index(drop=True)\n",
    "\n",
    "print(\n",
    "    \"X_scaled shape:\", X_scaled.shape,\n",
    "    \"| max abs:\", float(np.nanmax(np.abs(X_scaled)))\n",
    ")\n",
    "\n",
    "# 4) Fit PCA and check components.\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "assert np.isfinite(pca.components_).all(), \"Non-finite value in PCA components.\"\n",
    "\n",
    "# 5) Transform inside a local error-state context to silence spurious warnings.\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\", over=\"ignore\"):\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "\n",
    "df_pca = pd.DataFrame(X_pca, columns=[\"pc1\", \"pc2\"])\n",
    "expl_var = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Optional subsample for plotting (use same sample for k-means viz if you want).\n",
    "plot_sample = df_pca\n",
    "if len(plot_sample) > 8000:\n",
    "    plot_sample = plot_sample.sample(8000, random_state=0)\n",
    "\n",
    "# Use the styled helper defined in the cell above\n",
    "plot_pca_scatter(plot_sample[[\"pc1\", \"pc2\"]].to_numpy(), expl_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 10. Unsupervised clustering of segments\n",
    "\n",
    "As a simple check for natural acoustic regimes, we run k-means on the standardized acoustic features and visualize the clusters in the PCA space. Clusters that do not align with our expectations (for example clear speech-like vs music-like vs noise-like groups) may indicate interesting structure or domain shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "n_clusters = 3\n",
    "\n",
    "# Extra sanity check (will raise if something is truly wrong)\n",
    "assert np.isfinite(X_scaled).all(), \"Non-finite values in X_scaled for k-means.\"\n",
    "\n",
    "with np.errstate(divide=\"ignore\", over=\"ignore\", invalid=\"ignore\"):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\")\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_pca_clusters = df_pca.copy()\n",
    "df_pca_clusters[\"cluster\"] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(\n",
    "    data=df_pca_clusters,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"husl\",\n",
    "    s=12,\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"k-means clusters in PCA space\")\n",
    "plt.legend(title=\"cluster\", loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "display(cluster_counts.to_frame(name=\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the clusters.\n",
    "\n",
    "cluster_summary = (\n",
    "    df_acoustic.assign(cluster=cluster_labels)[\n",
    "        [\"cluster\", \"rms_db\", \"silence_ratio\", \"zero_crossing_rate\", \"snr_db\", \"energy_variance\"]\n",
    "    ]\n",
    "    .groupby(\"cluster\")\n",
    "    .agg([\"mean\", \"std\"])\n",
    ")\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Summarize the acoustic results from the clusters to check your understanding.\n",
    "\n",
    "+ `Cluster 0`:\n",
    "+ `Cluster 1`:\n",
    "+ `Cluster 2`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 11. Waveform and spectrogram views for individual segments\n",
    "\n",
    "Finally, we visualize raw audio for selected segments as both waveforms and log-mel spectrograms. This helps connect acoustic statistics (for example `rms_db`, `silence_ratio`, `snr_db`) with the underlying time–frequency patterns that SMAD models will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def plot_waveform_and_spectrogram(\n",
    "    audio_path: Path,\n",
    "    sample_rate: int = 16_000,\n",
    "    n_fft: int = 512,\n",
    "    hop_length: int = 160,\n",
    "    n_mels: int = 64,\n",
    ") -> None:\n",
    "    \"\"\"Plot waveform and log-mel spectrogram for a single segment.\n",
    "\n",
    "    The waveform is plotted with symmetric y-limits for easier comparison\n",
    "    across segments. The log-mel spectrogram uses time in seconds on the\n",
    "    x-axis instead of frame index.\n",
    "    \"\"\"\n",
    "    waveform, sr = load_waveform(audio_path)\n",
    "    sr = sr or sample_rate\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Waveform time axis\n",
    "    t = torch.arange(waveform.shape[-1]) / sr\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 4), sharex=False)\n",
    "\n",
    "    # Waveform\n",
    "    max_abs = float(waveform.abs().max())\n",
    "    axes[0].plot(t.numpy(), waveform.numpy(), linewidth=1.0)\n",
    "    axes[0].set_ylim(-1.05 * max_abs, 1.05 * max_abs)\n",
    "    axes[0].set_title(f\"Waveform: {audio_path.name}\")\n",
    "    axes[0].set_xlabel(\"time (s)\")\n",
    "    axes[0].set_ylabel(\"amplitude\")\n",
    "\n",
    "    # Mel spectrogram\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_db = torchaudio.functional.amplitude_to_DB(\n",
    "        mel_spec, multiplier=10.0, amin=1e-10, db_multiplier=0.0\n",
    "    )\n",
    "\n",
    "    mel_db_np = mel_db.numpy()\n",
    "\n",
    "    # Time axis in seconds for spectrogram\n",
    "    spec_frames = mel_db_np.shape[-1]\n",
    "    spec_duration = spec_frames * hop_length / sr\n",
    "    extent = [0.0, spec_duration, 0, n_mels]\n",
    "\n",
    "    im = axes[1].imshow(\n",
    "        mel_db_np,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "        extent=extent,\n",
    "        cmap=\"magma\",\n",
    "    )\n",
    "    axes[1].set_title(\"Log-mel spectrogram\")\n",
    "    axes[1].set_xlabel(\"time (s)\")\n",
    "    axes[1].set_ylabel(\"mel bin\")\n",
    "    fig.colorbar(im, ax=axes[1], shrink=0.8, label=\"dB\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_random_segment(df, query: str | None = None):\n",
    "    subset = df.query(query) if query else df\n",
    "    if subset.empty:\n",
    "        print(\"No rows matched the query.\")\n",
    "        return\n",
    "\n",
    "    row = subset.sample(1).iloc[0]\n",
    "    audio_path = _resolve_segment_path(row)\n",
    "\n",
    "    # Metadata block header\n",
    "    # print(f\"\\n=== Segment metadata: {audio_path.name} ===\")\n",
    "\n",
    "    # Show as a 1-row DataFrame (nice tabular view)\n",
    "    # row_df = row.to_frame().T\n",
    "    # display(row_df)\n",
    "\n",
    "    # Plot\n",
    "    plot_waveform_and_spectrogram(audio_path)\n",
    "\n",
    "    # Print text row for the record.\n",
    "    print()\n",
    "    print(row.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usages:\n",
    "\n",
    "# Any random segment\n",
    "visualize_random_segment(df_acoustic)\n",
    "\n",
    "# A segment with high silence_ratio (if that column exists)\n",
    "# visualize_random_segment(df_acoustic, query=\"silence_ratio > 0.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 12. Summary and next steps\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Loaded the BLOCS SMAD segment and acoustic manifests and verified that the segmentation looks sane.\n",
    "- Inspected acoustic QC flags (too quiet, mostly silence, heavily clipped, too short, had error) and their prevalence.\n",
    "- Explored distributions of core acoustic features and listened to random segments to connect the statistics to what we hear.\n",
    "- Examined correlations, 2D relationships, PCA structure, and simple k-means clusters to see how segments are arranged in acoustic space.\n",
    "- Added a placeholder view of gold label distributions that will activate once `label_source == \"gold\"` rows are available.\n",
    "\n",
    "Once you are comfortable with the acoustic and segmentation behavior, the recommended next step is:\n",
    "\n",
    "- Open `02_ast_teacher_sanity.ipynb` to analyze how AST, Whisper-AT, and CLAP-family teachers behave on BLOCS segments, compare them against gold labels (when available), and calibrate confidence thresholds for pseudo-labeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
